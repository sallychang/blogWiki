# 爬虫快速入门

## requests

### 参考资料

* [中文站](https://requests.readthedocs.io/zh_CN/latest/)


### 1 快速入门

在requests中，它会使用urllib3来自动完成Keep-alive和自动保持HTTP连接等功能。

* 使用`requests.get(url)`即可发送一个get请求
* 使用`requests.post(url,data={key,value})`可以发送一个POST请求。

同理，put,delete,head与options都可以这样发送。

#### 传递参数

有时会需要向URL的查询字符串中传递某种数据。如果是手工构建的话，需要向URL中插入键值对，并进行转码，例如`httpbin.org/get?key=val`

在requests中，可以使用params参数进行，例如`payload = {'key1': 'value1', 'key2': 'value2'}`,`r = requests.get("http://httpbin.org/get", params=payload)`。

也可以将多个值用列表形式传入：
```python
>>> payload = {'key1': 'value1', 'key2': ['value2', 'value3']}

>>> r = requests.get('http://httpbin.org/get', params=payload)
>>> print(r.url)
http://httpbin.org/get?key1=value1&key2=value2&key2=value3
```

##### 定制请求头

如果需要添加HTTP头部，则要传递一个dict给对应方法的headers参数，比如需要指定`content-type`时：
```python
>>> url = 'https://api.github.com/some/endpoint'
>>> headers = {'user-agent': 'my-app/0.0.1'}

>>> r = requests.get(url, headers=headers)
```

header的值必须是string、bytestring或者unicode。但是不建议传递unicode header

##### 传递复杂的POST请求

比如表单之类的，需要传递一个字典给data参数：
```python
>>> payload = {'key1': 'value1', 'key2': 'value2'}

>>> r = requests.post("http://httpbin.org/post", data=payload)
>>> print(r.text)
{
  ...
  "form": {
    "key2": "value2",
    "key1": "value1"
  },
  ...
}
```

如果是同一个key下的多个元素，可以使用元组列表：
```python
>>> payload = (('key1', 'value1'), ('key1', 'value2'))
>>> r = requests.post('http://httpbin.org/post', data=payload)
>>> print(r.text)
{
  ...
  "form": {
    "key1": [
      "value1",
      "value2"
    ]
  },
  ...
}
```

如果发送数据并非编码为表单形式，可以传递一个string。比如如果对方接受JSON格式数据，可以这样`payload = {'some': 'data'}`,`r = requests.post(url, data=json.dumps(payload))`。

当然，也可以直接用json参数传递`r = requests.post(url, json=payload)`，新版本会自动完成编码。

#### 响应内容

可以在调用上述方法后通过requests.models.Response对象来获得服务器响应的内容。

* r.text，可以获得unicode编码的内容。（比如对于网页而言，会返回网页的源码）
* r.encoding,通过这个属性，可以显示并改变r.text的编码。（每次访问r.text时，Requests都会使用新值进行解码）可以使用自己的编码，并用codecs模块进行注册
* r.content，使用字节的方式访问请求响应体（返回的字符串会带上b）。Requests会自动为你解码gzip和defalte传输编码的数据，例如使用`i = Image.open(BytesIO(r.content))`可以直接得到图片文件。
* r.json()：这是一个方法，可以调用Requests内置的JSON解码器来处理JSON数据。正常情况下会返回一个UTF-8编码的字典，如果没有JSON数据会报错（JSONDecodeError）；根据官网说法，解码错误还会报ValueError。解码正确也并不一定响应成功，因为有些服务器在失败的响应中同样是含有JSON对象的，需要进行检查
* r.raw：该属性包含了最原始的套接字响应。获取该属性**必须**要在初始请求中设置`stream=True`。可以使用`r.raw.read(n)`来读取前n个字节。但是一般情况是将文本流保存到文件读取：
```python
with open(filename, 'wb') as fd:
    for chunk in r.iter_content(chunk_size):
        fd.write(chunk)
```

此外，还有一些方便的属性
* r.headers可以拿到响应的请求头，形式时字典形式，仅为HTTP头部工作，字段不区分大小写。
* r.status_code，返回整数形式的状态码。（Requests有内置的状态码查询对象，如`requests.codes.ok`，有在面对错误请求时抛出异常的方法`r.raise_for_status()`，这个方法只在非200时报错）
* r.cookies，是一个字典，可以通过`cookie_name`来得到cookie的值。发送时可以使用cookies参数来在对应方法中发送

#### Cookie
requests中的RequestsCookieJar是一个功能强大的对象，接口比字典更完整，适合跨域名跨路径使用。
``` python
>>> jar = requests.cookies.RequestsCookieJar()
>>> jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')
>>> jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere')
>>> url = 'http://httpbin.org/cookies'
>>> r = requests.get(url, cookies=jar)
>>> r.text
'{"cookies": {"tasty_cookie": "yum"}}'
```

#### 重定向

默认情况下，Requests会处理除了HEAD以外的重定向。可以使用`r.history`来追踪重定向，这会返回一个列表，按照从最老到最新的请求进行排序。

可以在方法中使用`allow_redirects`参数来控制重定向是否允许发生。

#### 超时

可以告诉requests在`timeout`参数设定的秒数之后停止响应。基本上所有生产代码都应该使用这一擦拭农户，否则可能程序将永久失去响应。

到达时间后，方法会抛出`requests.exceptions.Timeout`错误。

注意：timeout仅对连接过程有效而与响应体的下载无关。即如果服务器在timeout秒内没有应答（没有从基础套接字上接收任何字节的数据），将会引发一个异常。

#### 错误与异常

* 遇到网络问题（DNS查询失败，拒绝连接）会抛出：`ConnectionError`异常
* 如果HTTP请求返回了不成功的状态码，`Response.raise_for_status()`会抛出`HTTPError`异常
* 如果请求超时，会抛出一个`Timeout`异常
* 如果请求超出了设定的最大重定向次数，会抛出一个`TooManyRedirects`异常

所有Requests显式抛出的异常都继承自`requests.exceptions.RequestException`中。

### 高级用法

#### 会话对象

会话对象可以在同一个Session实例发出的所有请求之间保持cookie。会复用底层的TCP连接，因此如果向同一个主机发送多个请求的话效率会有所提升。

Session的使用可以跨请求保持Cookie，也可以为请求方式提供缺省数据（通过为会话对象的属性提供数据来实现，如下）：

```python
s = requests.Session()
s.auth = ('user', 'pass')
s.headers.update({'x-test': 'true'})

# both 'x-test' and 'x-test2' are sent
s.get('http://httpbin.org/headers', headers={'x-test2': 'true'})
```

传递给请求方法的字典会与已经设置的会话层数据合并，方法的参数会覆盖会话的参数。

注意：方法级别的参数不会被保持

```python
s = requests.Session()

r = s.get('http://httpbin.org/cookies', cookies={'from-my': 'browser'})
print(r.text)
# '{"cookies": {"from-my": "browser"}}'

r = s.get('http://httpbin.org/cookies')
print(r.text)
# '{"cookies": {}}'
```

#### 请求与响应对象

在进行类似requests.get()的调用时，进行了两项操作，一项时构建了一个Request对象，另一项工作是一旦requests得到了一个从服务器返回的响应，会产生对应的Response对象，包含之前那个request对象。

因此，访问请求头也可以这么访问`r.requests.headers`

#### 准备的请求Prepared Request

#### SSL证书认证

Requests可以为HTTPS请求验证SSL证书，默认是开启的。如果整数验证失败会抛出`SSLError`。（或者在方法中指定`verify=True`）

可以使用`verify`参数来传入CA_BUNDLE文件的路径，或者包含可信任CA证书文件的文件夹路径。

#### 客户端证书

可以指定一个本地证书作为客户端证书。`requests.get('https://kennethreitz.org', cert=('/path/client.cert', '/path/client.key'))`

#### 响应体内容工作流

默认情况下，一旦进行网络请求，响应体就会被下载。可以通过`stream`参数覆盖这个行为，推迟下载响应体直到访问`Response.content`属性：`r = requests.get(tarball_url, stream=True)`

此时，requests只会下载响应头。

可以使用`Response.iter_content`和`Response.iter_lines`来控制工作流或者用`Response.raw`直接读取未解码的响应体。

#### 流式上传

Requests支持流式上传，此时上传大文件时就不需要先将它们全部读入内存。

```python
with open('massive-body') as f:
    requests.post('http://some.url/streamed', data=f)
```

PS：官方强烈建议使用二进制打开文件，不然可能会因为Content-Length设置问题导致错误。

#### 事件挂钩

Requests有一个钩子系统，可以用来操控部分请求过程，或者信号事件处理。

可以通过传递一个`{hook_name : callback_function}`字典给`hooks`请求参数为每个请求分配一个钩子函数。

例如`requests.get('http://httpbin.org', hooks=dict(response=print_url))`

callback如下：（会接受一个数据块作为它的第一个参数）
```python
def print_url(r, *args, **kwargs):
    print(r.url)
```

#### 流式请求

使用`Response.iter_lines()`可以很方便地应对流式API，即设置`stream=True`后使用`iter_lines`进行相应的迭代

```python
import json
import requests

r = requests.get('http://httpbin.org/stream/20', stream=True)

for line in r.iter_lines():

    # filter out keep-alive new lines
    if line:
        decoded_line = line.decode('utf-8')
        print(json.loads(decoded_line))
```

#### 代理

如果需要使用代理，则要为任意的请求方法提供`proxies`参数来配置单个请求。

```python
import requests

proxies = {
  "http": "http://10.10.1.10:3128",
  "https": "http://10.10.1.10:1080",
}

requests.get("http://example.org", proxies=proxies)
```

也可以配置系统环境变量`HTTP_PROXY`和`HTTPS_PROXY`来完成代理的配置。（此时就不用显式指明）

HTTP Basic Auth:
```python
proxies = {
    "http": "http://user:pass@10.10.1.10:3128/",
}
```

若要为某个特定的连接方式或主机设置代理，则使用`scheme://hostname`作为key。
`proxies = {'http://10.20.1.128': 'http://10.10.1.10:5323'}`，注意代理URL必须包含连接方式。

除了基本的HTTP代理，Requests还支持SOCKS协议代理。需要安装第三方库`pip install requests[socks]`

代理与HTTP类似：
```python
proxies = {
    'http': 'socks5://user:pass@host:port',
    'https': 'socks5://user:pass@host:port'
}
```


## beautifulSoup

beautifulSoup是python下的一个HTML或XML的解析库，所以首先要用requests这类网络访问库先拿到HTML文件。

其优势在于提供了python式的函数来处理导航、搜索和修改分析树等功能，比较方便。

### 参考文献

[官方文档](https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/)

### 对象种类

可以分为四种：
* Tag：与XML或HTML原生文档中的tag相同。
  * Name：每个`Tag`都有自己的名字，通过`.name`来获取。对该属性的修改会反映在文档中（标签发生改变）
  * Attributes：一个Tag可能有很多个属性，可以使用类似字典的方式访问。有些属性是单值的，访问后返回一个值；有些属性是多值的，访问后返回一个列表。（也可以直接使用`.attrs`来获取所有属性，返回一个字典）修改同样会起作用。
* NavigableString，用来包装tag中的字符串`tag.string`。表现上与Unicode字符串相同，也可以使用`unicode()`方法直接完成转换。tag中包含的字符串不能编辑，但是可以被替换成其它的字符串，用`replace_with()`方法。（我没有想明白，直接对string进行修改是可以的，这个不能编辑指的是什么）
* BeautifulSoup，该对象表示的是一个文档的全部内容，在大部分时候都可以将它当作一个`Tag`对象。其`name`属性比较特殊，为`[document]`
* Comment，注释及特殊字符串。BeautifulSoup定义的其他类型都可能会出现在XML中，与Comment类似，都是NavigableString的子类。

### 遍历文档树

#### 获取子节点

对于给定的标签tag

* `.contents`方法可以获得`tag`的所有直接的子节点，以列表的方式输出。
* `.children`为一个列表生成器
* `.descendants`会按照遍历形式展开**所有**的子节点（即按照在文档中从上到下的顺序进行枚举）
* `.string`，如果`tag`只有一个`NavigableString`类型的子节点，则可以用该方法获得字符串子节点。
* `.strings`，如果有多个字符串可以用该方法获得
* `.stripped_strings`，如果输出的字符串中可能包含了很多空格或空行，可以使用这个方法来去除多余空白内容。

#### 获取父节点

* `.parent`，获取某个元素的父节点。BeautifulSoup的父节点是None。
* `.parents`，可以递归得到所有父辈结点（直到None），以列表生成器的形式呈现。

#### 获取兄弟结点

* `.next_sibling`，查找下一个同级的结点，没有则返回None
* `.previous_sibling`，查找上一个同级的结点，没有则返回None。
* `.next_siblings`，迭代输出
* `.previous_siblings`，迭代输出

#### 回退和前进

与前面部分的区别是element强调的是解析顺序，虽然没看明白

* `.next_element`
* `.previous_element`
* `.next_elements`
* `.previous_elements`

### 搜索文档树

比较常用的有两个：
* `.find()`
* `.find_all()`

可以搜索的过滤器有：
* 字符串。直接传入一个字符串，会搜索完全匹配的tag
* 正则表达式。会使用正则表达式的`match()`来匹配内容。比如`soup.find_all(re.compile("^b"))`就选择了所有以b开头的标签。
* 列表。如果传入列表参数，BeautifulSoup会将与列表中任一元素匹配的内容返回。
* `True`，可以匹配任何值，但不会返回字符串节点。
* 方法。可以传入一个自定义方法，方法只接受一个元素参数。如果方法返回True表示当前元素匹配并且找到；否则返回False。
```python
def has_class_but_no_id(tag):
    return tag.has_attr('class') and not tag.has_attr('id')
```

细节：
`find_all(name,attrs,recursive,string,**kwargs)`，缺省的字符串会赋值给name参数。

可以指定keyword参数来获得详细值，比如`soup.find_all(id='link2')`会查找所有指定id的标签。`soup.find_all(href=re.compile("elsie"))`会查找所有链接匹配正则的标签。

按CSS搜索，可以使用`class_`来匹配class：`soup.find_all("a", class_="sister")`。由于class是多值的，参数是单个的时候可以匹配到tag中多值的任意一值；如果是多值的时候只能进行完全匹配。

string参数，可以搜索文档中的字符串内容，与name参数接受的内容是一样的。

limit参数，限制返回的结果数量

recursive参数，值为False的时候只搜索直接子节点，默认为True。

其他类型的函数：
* `find_parents()`
* `find_parent()`
* `find_next_siblings()`
* `find_next_sibling()`
* `find_previous_siblings()`
* `find_previous_sibling()`
* `find_all_next()`，通过`next_elements`属性进行迭代
* `find_next()`
* `find_all_previous()`
* `find_previous`

##### CSS选择器

在`Tag`或者`BeautifulSoup`对象的`.select`方法中传入字符串参数，可以按照CSS选择器的语法找到tag。

如果只是需要CSS选择器的功能，可以直接使用`lxml`。

### 修改文档树

直接修改Tag的Attributes可以完成对指定属性的修改、添加或删除

修改`.string`，给tag的该属性赋值，既可以用当前内容来替代原来的内容

`tag.append()`方法，类似与列表中的同名方法，在结点中加入指定内容

`BeautifulSoup.new_tag()`工厂方法，可以创建新的标签。如`new_tag = soup.new_tag("a", href="http://www.example.com")`

都是插入方法。

* `insert()`，与append类似，区别是不会将新元素添加到父节点的`.contents`属性的最后，而是会插入到指定位置。
* `insert_before()`
* `insert_after()`

`tag.clear()`方法，移除当前tag的内容

`tag.extract()`方法，将当前tag移除文档树，并作为方法结果返回。会将tag连带里面的内容都移除出去

`tag.decompose()`，与前面那个类似。但是不会返回移除出去的结点，而是会删除这个结点。

`replace_with()`，该方法移除文档树中的某段内容，并用新的tag或文本结点替代它。

`wrap()`可以对指定的元素进行包装，并返回包装后的结果。
```python
soup = BeautifulSoup("<p>I wish I was bold.</p>")
soup.p.string.wrap(soup.new_tag("b"))
# <b>I wish I was bold.</b>

soup.p.wrap(soup.new_tag("div"))
# <div><p><b>I wish I was bold.</b></p></div>
```

`unwrap()`，与`wrap()`方法相反，移除tag内的所有tag标签。常用于标记的解包
```python
markup = '<a href="http://example.com/">I linked to <i>example.com</i></a>'
soup = BeautifulSoup(markup)
a_tag = soup.a

a_tag.i.unwrap()
a_tag
# <a href="http://example.com/">I linked to example.com</a>
```

### 输出

格式化输出，使用`prettify()`方法，可以将BeautifulSoup的文档树格式化后以Unicode编码的形式输出。

压缩输出，即直接对BeautifulSoup对象或者Tag对象使用Python的unicode()或str()方法。

get_text()，可以直接得到tag中包含的文本内容，包括子孙tag中的内容，并将结果作为Unicode返回。

