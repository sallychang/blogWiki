# 爬虫快速入门

## requests

### 参考资料

* [中文站](https://requests.readthedocs.io/zh_CN/latest/)


### 1 快速入门

在requests中，它会使用urllib3来自动完成Keep-alive和自动保持HTTP连接等功能。

* 使用`requests.get(url)`即可发送一个get请求
* 使用`requests.post(url,data={key,value})`可以发送一个POST请求。

同理，put,delete,head与options都可以这样发送。

#### 传递参数

有时会需要向URL的查询字符串中传递某种数据。如果是手工构建的话，需要向URL中插入键值对，并进行转码，例如`httpbin.org/get?key=val`

在requests中，可以使用params参数进行，例如`payload = {'key1': 'value1', 'key2': 'value2'}`,`r = requests.get("http://httpbin.org/get", params=payload)`。

也可以将多个值用列表形式传入：
```python
>>> payload = {'key1': 'value1', 'key2': ['value2', 'value3']}

>>> r = requests.get('http://httpbin.org/get', params=payload)
>>> print(r.url)
http://httpbin.org/get?key1=value1&key2=value2&key2=value3
```

##### 定制请求头

如果需要添加HTTP头部，则要传递一个dict给对应方法的headers参数，比如需要指定`content-type`时：
```python
>>> url = 'https://api.github.com/some/endpoint'
>>> headers = {'user-agent': 'my-app/0.0.1'}

>>> r = requests.get(url, headers=headers)
```

header的值必须是string、bytestring或者unicode。但是不建议传递unicode header

##### 传递复杂的POST请求

比如表单之类的，需要传递一个字典给data参数：
```python
>>> payload = {'key1': 'value1', 'key2': 'value2'}

>>> r = requests.post("http://httpbin.org/post", data=payload)
>>> print(r.text)
{
  ...
  "form": {
    "key2": "value2",
    "key1": "value1"
  },
  ...
}
```

如果是同一个key下的多个元素，可以使用元组列表：
```python
>>> payload = (('key1', 'value1'), ('key1', 'value2'))
>>> r = requests.post('http://httpbin.org/post', data=payload)
>>> print(r.text)
{
  ...
  "form": {
    "key1": [
      "value1",
      "value2"
    ]
  },
  ...
}
```

如果发送数据并非编码为表单形式，可以传递一个string。比如如果对方接受JSON格式数据，可以这样`payload = {'some': 'data'}`,`r = requests.post(url, data=json.dumps(payload))`。

当然，也可以直接用json参数传递`r = requests.post(url, json=payload)`，新版本会自动完成编码。

#### 响应内容

可以在调用上述方法后通过requests.models.Response对象来获得服务器响应的内容。

* r.text，可以获得unicode编码的内容。（比如对于网页而言，会返回网页的源码）
* r.encoding,通过这个属性，可以显示并改变r.text的编码。（每次访问r.text时，Requests都会使用新值进行解码）可以使用自己的编码，并用codecs模块进行注册
* r.content，使用字节的方式访问请求响应体（返回的字符串会带上b）。Requests会自动为你解码gzip和defalte传输编码的数据，例如使用`i = Image.open(BytesIO(r.content))`可以直接得到图片文件。
* r.json()：这是一个方法，可以调用Requests内置的JSON解码器来处理JSON数据。正常情况下会返回一个UTF-8编码的字典，如果没有JSON数据会报错（JSONDecodeError）；根据官网说法，解码错误还会报ValueError。解码正确也并不一定响应成功，因为有些服务器在失败的响应中同样是含有JSON对象的，需要进行检查
* r.raw：该属性包含了最原始的套接字响应。获取该属性**必须**要在初始请求中设置`stream=True`。可以使用`r.raw.read(n)`来读取前n个字节。但是一般情况是将文本流保存到文件读取：
```python
with open(filename, 'wb') as fd:
    for chunk in r.iter_content(chunk_size):
        fd.write(chunk)
```

此外，还有一些方便的属性
* r.headers可以拿到响应的请求头，形式时字典形式，仅为HTTP头部工作，字段不区分大小写。
* r.status_code，返回整数形式的状态码。（Requests有内置的状态码查询对象，如`requests.codes.ok`，有在面对错误请求时抛出异常的方法`r.raise_for_status()`，这个方法只在非200时报错）
* r.cookies，是一个字典，可以通过`cookie_name`来得到cookie的值。发送时可以使用cookies参数来在对应方法中发送

#### Cookie
requests中的RequestsCookieJar是一个功能强大的对象，接口比字典更完整，适合跨域名跨路径使用。
``` python
>>> jar = requests.cookies.RequestsCookieJar()
>>> jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')
>>> jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere')
>>> url = 'http://httpbin.org/cookies'
>>> r = requests.get(url, cookies=jar)
>>> r.text
'{"cookies": {"tasty_cookie": "yum"}}'
```

#### 重定向

默认情况下，Requests会处理除了HEAD以外的重定向。可以使用`r.history`来追踪重定向，这会返回一个列表，按照从最老到最新的请求进行排序。

可以在方法中使用`allow_redirects`参数来控制重定向是否允许发生。

#### 超时

可以告诉requests在`timeout`参数设定的秒数之后停止响应。基本上所有生产代码都应该使用这一擦拭农户，否则可能程序将永久失去响应。

到达时间后，方法会抛出`requests.exceptions.Timeout`错误。

注意：timeout仅对连接过程有效而与响应体的下载无关。即如果服务器在timeout秒内没有应答（没有从基础套接字上接收任何字节的数据），将会引发一个异常。

#### 错误与异常

* 遇到网络问题（DNS查询失败，拒绝连接）会抛出：`ConnectionError`异常
* 如果HTTP请求返回了不成功的状态码，`Response.raise_for_status()`会抛出`HTTPError`异常
* 如果请求超时，会抛出一个`Timeout`异常
* 如果请求超出了设定的最大重定向次数，会抛出一个`TooManyRedirects`异常

所有Requests显式抛出的异常都继承自`requests.exceptions.RequestException`中。

## beautifulSoup

