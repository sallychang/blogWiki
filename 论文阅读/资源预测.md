# 资源预测

要求：
* 穷尽所有进行资源预测方面的论文，详细探索所用到的技术、方法、数据、实验环境等。
* 总结提出的问题与解决方案，并且提出在多服务相关方面的问题。

* [Using ELM techniques to predict data center VM requests]

## 文献探索参考

### 综述文献

#### 综述文献1

[Survey on prediction models of applications for resources provisioning in the cloud](论文阅读/资源预测/Survey%20on%20prediction%20models%20of%20applications%20for%20resources%20provisioning%20in%20the%20cloud.md)

* 综述Survey on prediction models of applications for resources provisioning in the cloud，Table2的Resources Utilization、Future Demand of Resources都是可以的，其他一些比如The number of VMs、The number of PMs也可以作为参考。
* Table3 中使用CPU和内存利用率进行预测的部分。同时也可以参考一下响应时间的这部分。
* 3.4 Machine Learning techniques中的部分，4.1 Methods based on machine learning and statistical techniques中的相关部分。
* 4.3节提到有人使用AR模型，将CPU share映射到响应时间，从而使用控制论来进行性能预测。

#### 综述文献2

[Elasticity in Cloud Computing : State of the Art and Research Challenges](/论文阅读/综述文献阅读/Elasticity%20in%20Cloud%20Computing_State%20of%20the%20Art%20and%20Research%20Challenges.md)

* 从预测对象上来说，所有Proactive中使用时间序列预测的都是可以考虑的，因为要么是流量预测，要么是资源预测。
* 同时也可以考虑2.2.3的Purpose，一般来说，从节省消耗、能源出发都是需要预测的。

#### 综述文献3

Quality-of-service in cloud computing: modeling techniques and their applications

这个是关于应用建模的综述论文，可以看一下，但是不要抱有太多的期望，因为它主要还是从排队论角度出发的，而排队论所要求的泊松流在多服务中会因为上游结点阻塞导致不满足条件。

#### 综述文献4

Cloud computing: State-of-the-art and research challenges，2010年的综述论文，需要谨慎对待。

#### 综述文献5

Auto-Scaling Web Applications in Clouds : A Taxonomy and Survey，2018年的综述论文，还不错
* 第8和第10部分都很不错，有一些比较不错的文章。
Research on Auto-Scaling of Web Applications in Cloud: Survey, Trends and Future Directions，2019年的综述论文，还不错
* 第五部分，Survey on Auto-scaling Techniques。这篇主要是从资源角度来说的。

给出了一个Resource Estimation的定义，目标是确定最小的计算资源来处理当前的流量从而决定如何执行调度动作。

有一个很不错的地方是在其8.4 Analytical Modeling这里，提到了两种解决多层应用排队网络问题的想法，一个是SLA分解，一个是统一规划

综述：88、85

#### 综述文献5

A Review of Auto-scaling Techniques for Elastic Applications in Cloud Environments，保持谨慎态度

* 第五部分

该文章对于统一测试工具的提出还是有一定帮助的。

#### 综述文献6

Prediction methods for effective resource provisioning in cloud computing: A survey，这篇文章内提到的文章都可以去详细综述一下。比较可惜的是这篇文章是关于VM的，因此其中很多方法其实并不适用。

本文关注的是预测方法，解决对云应用的流量预测问题，并进行提前的分配，从而更好地满足SLO同时减少消耗。

个人注意有一个点，一般对于resource workload behaviour，除了一般提到的request arrival rate，还有一个service time distribution(这个一般采用日志系统获取)

一些定义：
* Resource provisioning:将一系列流量分配给一个同构资源集合，将最佳的资源分配给指定的流量。
* Resource scheduling：由Resource allocation和Resource Mapping构成。
* Resource Allocation的目的是将合适的资源及时分配给指定流量，或者说，最小化分配的资源量使得刚好满足SLA。
* Resource Mapping是将workload在基于QoS的条件下映射到合适资源的过程。

## 快速阅读

#### The placement method of resources and applications based on request prediction in cloud data center

首先使用请求数预测来提前得到未来的请求数，然后采用了基于Utility Ratio Matrix的方法来配置资源。

目标是为了节省云数据中心的能量消耗。

* 构建了一个基于流量预测的reconfiguration framework
* 为了避免delayed reconfiguration，使用了基于Modifying Index Curve Model的方法
* 使用算法AVMR来重新配置VM和其他资源。这个算法能够基于流量预测值从实际的reconfiguration中分离具体的配置项。

预测方法采用了名为Modified Index Curve Model来预测未来应用的流量。以最优化的思维来解决

数据为FJUT数据中心一天的用户访问量，

#### A cost-aware auto-scaling approach using the workload prediction in service clouds

预测式水平扩缩与基于阈值法的垂直扩缩相结合

调度的时候分为三层，self-healing/resource-level scaling是垂直扩缩，VM-level是水平扩缩。

一些细节设定：预测的时间间隔与VM重启的时间是相同的。

比较奇怪的是它将线性回归方法与ARMA做了对比，结果显示线性回归更好。我倾向于认为它所训练模型的时间比较小，因此ARMA没有办法很好地矫正参数。而且确实如它所说，ARMA总是会有滞后、略小的情况，这在预测中是很致命的

self-healing调度是在虚拟机之间交换虚拟资源，因为作者使用的虚拟框架原因，基本不耗费任何代价

因为不同的VM根据规格不同，能处理的流量也是不一样的，因此使用VRC虚拟资源消耗来进行VM-level的扩缩。

VRC包括商业软件的license fee和虚拟资源的

冷启动；使用经验预测初始流量

调度是分别对每个个体进行调度的，会更简单一些

垂直扩缩起到的作用是微调，只在小于上阈值的时候执行；水平扩缩才会在超过阈值的时候执行。

求解具体资源的方式是使用类似线性规划的思维，确定数个等式、数个约束条件和一个目标，解得使得这个目标最小的资源数量。

#### A Predictive Method for Workload Forecasting in the Cloud Environment

从能量简约角度出发，考虑更高的资源利用率。使用神经网络参与预测访问量。

TDNN（Time Delay Neural Network），前向神经网络，使用time delay information，来进行预测。

本文的workload定义包含运行的应用数量与连接的用户数量，本文注重于计算机中运行的应用数量。使用RNN来进行预测。

构建了一个resource manager，负责resource allocation problem1

很普通的一篇论文，采用RNN来预测流量并与TDNN进行对比，只进行了模拟实验，也没有提如何完成流量到资源的对应。垃圾。

#### A mixture of HMM,GA and Elman network for load prediction in cloud-oriented data centers

想法是使用HMM自动聚类。即自动发现流量的模式，进行分类，并基于此来完成预测。

然后使用GA-Elman神经网络进行短期流量预测，本质上还是RNN预测，不过使用了GA来进行自动地参数和阈值选择。

聚类方法；使用固定长度的时间窗口，提取特征。每12个小时重新训练一次。

测试的数据源来自谷歌集群，使用了一周的数据；同时使用了NASA的数据

模拟实验的对象是CPU占用率，与GM(1,1)，九阶AR，MA，指数平滑，BP和PF、ANFIS进行了对比。

只进行了模拟实验，因此也没有涉及怎样从预测结果指向真实调度动作

#### A new method based on PSR and EA-GMDH for host load prediction in cloud computing system

预测算法，结合了Phase Space Reconstruction method 和Group Method of Data Handling based on an Evolutionary Algorithm

测试数据使用了一个分布式数据中心的日志，以及一个google集群的数据。

PSR是local prediction中的重要方法，用一系列变量来重建原始流量。

EA-GMDH是一个新的GMDH型的神经网络

#### A Novel VM Workload Prediction using Grey Forecasting Model in Cloud Data Center

#### Predicing cloud resource provisioning using machine learning technique

数据来源是使用TPC-W基准测试工具收集到的数据。

#### PRESS： Predictive Elastic Resource Scaling for cloud systems

对于重复的工作负载，PRESS能够提取出流量的模式，并在预测中使用这个模式。这些模式通常源自重复请求或是循环迭代。

使用信号处理技术来获取模式长度（FFT），选择最低的主频。然后使用Pearson、DTW等来进行模式匹配。

对于没有重复模式的流量，使用离散时间的马尔科夫链，固定数量状态，来进行短期预测。（即学习出一个概率转移矩阵）

通过结合上述两种方式，可以从头开始运行一个应用。

PRESS在实验中会每分钟进行一次预测，根据这个预测的结果来分配资源（用类似垂直调度的方式，调整CPU上限）

PRESS会对预测值进行一定数量的超分配，和我的想法一样

从最终任务的角度上：

capacity planning/ cloud resource provisioning

#### Cloud analytics for capacity planning and instant VM provisioning

预测资源过程中存在的困难：
1. The resource demands are highly unstable:两个原因，一个是用户本身的构成很复杂，资源获取和释放的方式也很随机。不同的虚拟机资源可能会包含不同的随机因素，从而影响到最终的结果。
2. 为了实现实时的调度，我们需要去预测每一个VM型号的需求，并提前准备。

#### Learning Predictive Autoscaling Policies for Cloud-hosted Microservices Using Trace-driven Modeling

从摘要来看，这篇文章的核心并不是落在预测流量上，而是在决定合适的阈值法数值。用原文的话来说，就是determine the most appropriate threshold values to minimize resource consumption and performance violations.

该文对比的预测式算法为
* Predictive auto-scaling of multi-tier applications using performance varing cloud resources. 2019
* An architecture for automatic scaling of replicated services
* Unsupervised learning of dynamic resource provisioning policies for cloud-hosted multitier web applications.

从它给的示意图上，是先进行一个stress testing，基于workload performance tracing，完成response time modeling，最后进行一个trace-driven auto-sclaing simulation，使用decision tree regressor来基于预测的流量和用户定义的响应时间阈值分配资源

第一个response time modeling使用的是curve fitting technique，而且拟合的曲线其实误差还是挺大的，使用了一个指数函数来进行拟合。这里建立了一个流量到响应时间的映射关系。

使用决策树回归算法，根据流量与期望的响应时间预测指定的VM数量。

使用了快增慢缩策略，连续三次小于阈值的一半才对VM进行缩减。

响应式调度与预测式调度结合，响应时间建模进行的是快速的阈值法调度，决策树回归进行的是预测式调度。

#### Performance Management of Web Services Using Machine Learning Techniques

3.1 Response Time Prediction for Web Services(前面2.3部分相关响应时间预测的综述也值得一读)

考虑了到来的流量的速率与分布，分布中考虑了Hyper-Exponential和Hypo-Exponential两种

不过虽然论文考虑的是Web Service的预测，实际上更多是transaction的混合。

文章会对workload的特征进行建模，建模使用的依据是the load of system。作者的想法是不同的流量可能会对系统有着相同的压力。

如果流量在过去出现过，则使用CRE根据流量的分类来进行预测，要么直接找到流量，要么基于近似的流量完成预测，得到响应时间。

如果流量在过去没有出现过，

#### Adaptive cloud resource management through workload prediction

上面的Host Load Prediction与Application prediction的对比值得借鉴学习。

#### Towards resoure-Efficient Cloud systems: Avoiding Over-Provisioning in Demand-Prediction Based Resource Provisioning

作者注意到，如果不排除过去resource demand的时间序列中的一些burst，则在未来的预测中会出现padding，即申请的资源数量远大于实际需求，这个差值被称为padding。

文章采用了RPRP算法，在资源预测中exclude burst in demand prediction，并采用特殊的算法来处理这些burst，避免超分配的问题。不想是之前一样将padding设置为一个很高的值，RPRP自行采用一些方法来调整padding。

同时，为了在运行中处理burst，RPRP采用了响应式的算法来调整padding。实际上是一个基于概率论的动态规划，说实话没看明白。

实际的预测中直接用了FFT……

#### Proviidng Performance Guarantees for Cloud-Deployed Applications

使用卡尔曼滤波来自动学习应用的系统参数（这是什么技术）

使用排队网络来对多层应用进行建模（正常操作）。因为无法访问用户应用（基准测试、application profiling）来获得模型的具体参数，使用卡尔曼滤波器来推断一些不能观测出的参数。

给定SLA和每层的响应时间，可以直接计算出所需要的最小实例数量。

#### Resonse time-based resource allocation according to service level agreements in cloud computing

系统是通过保证在运行池中的VM的数量来使得满足SLA的目标

使用了排队论的想法，对p个VM集群进行建模，最终的目标是在满足SLA的前提下最小化总代价。

其中，响应时间的分位点（比如95分位点）被考虑为SLA的重要因素，因此本文是从响应时间角度出发的。

过程：
1. 基于资源选择，找到p个sites
2. 基于两个等式进行integer optimisation，要求服务可用率大于期望（即控制SLA违约率）
* 因为服务的响应时间的提升会影响到SLA违约的概率，因此暗示需要进行资源分配来满足SLA，因此availability factor也是被响应时间所决定的。

#### Predicting the end-to-end tail latency of containerized microservices in the cloud

基本考虑到了我所想要考虑的东西，k8s平台下的DAG架构，如何监控指标、如何监控流量结构、如何抓取日志等等。

使用机器学习来根据CPU占用率预测端到端延迟

方法；
1. 数据收集。使用CPU占用率作为微服务的资源指标，因为CPU占用率一般为主要的瓶颈资源。使用docker stats来检测pod level CPU utilization。使用Locust来监控端到端延迟。收集到的数据被用来训练基于机器学习的性能模型。
2. 机器学习性能模型。使用sklearn，首先选择特征(feature selection)，包括并行用户数量、pod级别指标和VM级别指标。pod级别指标包括服务级平均CPU占用率；VM级别指标包括VM的CPU占用率。为了缩小特征空间和避免过拟合，采用名为stability selection的方式进行特征选择，对训练数据进行降采样。

在预测结束后的具体调度上并没有采用用户定义阈值法的思想，因为阈值法为应用维护者带来了巨大的负担，一个设置不好的阈值会带来巨大的问题。算法会自动根据SLO决定阈值。

将问题转换为了一个最优化问题，在满足SLO的情况下最大化平均的CPU占用率。这里的CPU占用率指的是一个服务所有pod的占用率。服务之间的依赖关系可以用DAG获得，或者通过前面提到的机器学习方法中的特征选择部分得到。

#### An RNN-LSTM based Flavor Recommender Framework in Hybrid Cloud

这篇文章挺有意思的，目标是对给定的application推荐flavor to adjust capacity of resources at low possible cost，简而言之就是推荐资源。

分为三个部分：
1. 使用RNN+LSTM来预测未来的workload（CPU和内存占用率）
2. 推荐flavor，最优化代价的同时满足SLA
3. 将推荐的资源应用在未来的workload上，确保应用的高可用性（避免SLA违约）

在组成架构上，一个是Prediction Processing Engine，一个是scoring engine，这两个需要重点实现。

首先使用CPU和内存的预测值，得到每台机器的CPU和内存的上可能界限和下可能界限。基于这些值推荐配置进行打分，选出代价最低的配置。

#### Unsupervised Learning of Dynamic Resource Provisioning Policies for Cloud-Hosted Multitier Web Applications

首先，通过建模方式获得workload pattern，这个部分可以在运行时通过监控系统参数获得。

模型由两个部分组成：
* URI space partitioning：对应用的URI空间进行切分
* workload pattern：针对URI空间建立一个请求到达的概率模型。

URI切分是将具有相同资源特性的部分分为一类，最终分为k类（比如都是请求大量CPU资源、请求数据库，之类的）。在每一类中，对每一个资源的请求是随机的，但是服从同一个分布。更进一步，一般来说，k是一个比较小的值。模型最终选用的是Gaussian mix cluster algorithm来进行分类，然后建立映射关系。

令C为一个随机变量，代表请求落在哪一个类中，P(C)这个概率分布函数即定义了一个workload distribution。

